<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>sigmoid prime: approachable machine learning</title>
    <meta name="description" content="Striving for intuitive explanations of machine learning concepts.">
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">

    <link rel="stylesheet" href="/css/style.css">
    <link rel="stylesheet" href="/libs/highlight/styles/atom-one-light.css">
    <link rel="stylesheet" href="/libs/katex/katex.min.css">
    <link rel="stylesheet" href="/libs/c3/c3.min.css">

    <link rel="apple-touch-icon-precomposed" sizes="57x57" href="/assets/favicons/apple-touch-icon-57x57.png">
    <link rel="apple-touch-icon-precomposed" sizes="114x114" href="/assets/favicons/apple-touch-icon-114x114.png">
    <link rel="apple-touch-icon-precomposed" sizes="72x72" href="/assets/favicons/apple-touch-icon-72x72.png">
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/assets/favicons/apple-touch-icon-144x144.png">
    <link rel="apple-touch-icon-precomposed" sizes="60x60" href="/assets/favicons/apple-touch-icon-60x60.png">
    <link rel="apple-touch-icon-precomposed" sizes="120x120" href="/assets/favicons/apple-touch-icon-120x120.png">
    <link rel="apple-touch-icon-precomposed" sizes="76x76" href="/assets/favicons/apple-touch-icon-76x76.png">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="/assets/favicons/apple-touch-icon-152x152.png">
    <link rel="icon" type="image/png" href="/assets/favicons/favicon-196x196.png" sizes="196x196">
    <link rel="icon" type="image/png" href="/assets/favicons/favicon-96x96.png" sizes="96x96">
    <link rel="icon" type="image/png" href="/assets/favicons/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/assets/favicons/favicon-16x16.png" sizes="16x16">
    <link rel="icon" type="image/png" href="/assets/favicons/favicon-128.png" sizes="128x128">

    <script src="/libs/highlight/highlight.pack.js"></script>
    <script src="/libs/katex/katex.min.js"></script>
    <script src="/libs/katex/auto-render.min.js"></script>
    <script src="/libs/d3/d3.v3.min.js"></script>
    <script src="/libs/c3/c3.min.js"></script>
    <script src="/libs/function-plot/function-plot.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
  </head>
  <body>
    <header class="top-header">
      <div class="inner-header">
        <div class="logo-container">
          <a href="/"><span class="logo" style="font-size:25px;">sigmoid<span class="prime" style="font-size:42px">′</span></span></a>
        </div>
      </div>
    </div>
    <main style="margin-top:0px;">
      <div class="article-container ">
        <article>
          <header>
            <h1 class="post-title">How to Build (and Understand) a Neural Network Pt. 3: The Forward Pass</h1>
          </header>
          <div class="date-ct">
            <p>By August Wester on 07 April 2017</p>
          </div>
          
<p>By now, we have covered a whole lot of material. We have talked about decision boundaries, cost functions, and gradient descent, and we have built our own linear models using the perceptron and logistic regression algorithms. In this penultimate post of the series, we will analyze the first half of a neural network — the <strong>forward pass</strong> — and we will use linear algebra to explore a visual interpretation of what happens to our data as it flows through a neural net.</p>

<h2 id="a-birds-eye-view">A bird’s-eye view</h2>
<p>This diagram shows a three-layered neural network composed of an <strong>input layer</strong> (green), two <strong>hidden layers</strong> (purple), and an <strong>output layer</strong> (red).</p>

<p><img src="/assets/posts/3/nn_2.png" width="450" /></p>

<ul>
  <li><strong>Input layer:</strong> The input layer doesn’t do much at all; it merely represents the input of raw data to the network. The input is always a numerical representation of a piece of data (e.g. a grayscale value between 0 and 1 representing the intensity of a pixel in an image).</li>
  <li><strong>Hidden layer:</strong> Above, we have two hidden layers, each consisting of three neurons. (The term ‘hidden’ is silly, and refers to nothing more than the fact that the layers lie between the input and the output of the network.) The job of each hidden neuron is to take its input and apply it to a mathematical function known as an <strong>activation function</strong>. The output of the activation function — the <strong>activation</strong> — is then passed on to the next layer in the network.</li>
  <li><strong>Output layer</strong>: The output layer is the last part of a  neural net. Each neuron in the output layer receives the full set of activations from the final hidden layer. On the basis of this, it then outputs a probabilistic estimate that enables us to infer something about the input data. In non-binary classification problems, there is one output neuron for each of the classes we wish to distinguish between. The output of each output neuron is the probability that a sample belongs to whatever class the output neuron represents.</li>
</ul>

<p>Besides indicating the transfer of data, the synapses (lines) in the diagram also represent weights. As with the perceptron and logistic regression algorithms, weights are crucial to how a neural net operates. Every time a value is passed from one neuron to the next, it is multiplied by a unique weight. When we first initialize a neural net, the weights are all chosen at random. However, as we train the network, we adjust the weights up or down with the help of gradient descent to make them all converge on a combination that approximates a desired end result.</p>

<p>In addition to the weights, there is also a unique bias coupled to each hidden neuron and each output neuron. The biases are added to the neurons’ sum of weighted inputs <em>before</em> the activation function is applied.</p>

<p>Once a neural net has been trained, it can be used to classify unseen samples by sending them through the network and looking at the result of the output neurons. Since there are no theoretical restrictions on the number of layers and neurons, a neural network can be used to approximate any mathematical function at all. We will see why later in this post.</p>

<h2 id="mnist">MNIST</h2>

<p>The most well-known dataset in the machine learning community (perhaps only rivalled by Iris) is called <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a>. MNIST contains 70,000 grayscale images of handwritten digits from 0-9. 60,000 of these are training samples, while the remaining 10,000 are test samples. Each image is 28x28 pixels, giving us 784 pixels per image. Because the images are all grayscale, each pixel can be represented as a floating point value between 0 and 1.</p>

<p><img src="/assets/posts/3/mnist.png" width="350" /></p>

<p>In a traditional neural network (like the one we are building today), we don’t preserve the dimensionality of the images by representing them as matrices. Instead we choose to <strong>unroll</strong> each one of them into a flat 784-dimensional vector €x€. This is in contrast to <a href="http://cs231n.github.io/convolutional-networks/">convolutional neural networks</a> that are specifically optimized for computer vision, and need to preserve the spatial locality of every pixel.</p>

<p>It is intuitive to think of €x€ as a regular image with only visual qualities. However, as machine learning practitioners, we need to accustom ourselves to thinking about our data in the way that our algorithms see it; namely as a collection of numbers.</p>

<p>When an image is viewed as a 784-dimensional vector, we can think of it as occupying a point in 784-dimensional space. When we do this, we find that images that are visually (<em>not</em> conceptually) similar group together and form clusters. In the case of MNIST, this means that each digit has its own cluster in the 784-dimensional space that the images inhabit.</p>

<p><img src="/assets/posts/3/tsnemnist.png" width="800" /></p>

<p class="subtext">30,000 MNIST images reduced from 784 dimensions to two dimensions using <a href="http://distill.pub/2016/misread-tsne/">t-SNE</a></p>

<p>At this point, a reasonable suggestion would be to use <a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm">k-nearest neighbor</a> to classify an image based on the cluster in which it is located. However, there is a myriad of problems with this approach: First, each digit has a fair amount of outliers (i.e. 5s that look like 6s). Second, most clusters are located very close to each other, which makes it hard to classify test samples that fall around the edges of a cluster. Additionally (and most importantly), with 60,000 training samples, it would be <em>extremely</em> computationally inefficient to calculate the Euclidean distance to each point before we could draw any conclusions about the data.</p>

<p>The million dollar question now is whether or not it is possible to minimize the amount of outliers, pull apart the clusters, and maintain computational efficiency. The answer to this question, of course, is “yes”, and we are going to demonstrate it using a neural network.</p>

<h2 id="the-forward-pass">The forward pass</h2>

<p>Before we get into the details of <em>why</em> neural nets work, it might be helpful to see the calculations involved in sending a <strong>mini-batch</strong> of 75 MNIST samples through a very simple, hypothetical network. This operation is called the <strong>forward pass</strong>.</p>

<p>Because we unroll our 28x28 images into a 784-dimensional vector, our network will have 784 input neurons. For the sake of keeping things simple, our hypothetical network will only have a single hidden layer consisting of three hidden neurons. This would not be enough for an actual neural network, but it will suffice for the purposes of this example.</p>

<p>Since we are looking to classify each sample as being a digit between 0-9, we will have 10 output neurons at the end of the network.</p>

<p>With 75 samples each containing 784 features, we can represent our mini-batch as a 784x75 matrix. Each column will thus contain the pixel values for a single image, while the rows contain the grayscale value for the same particular pixel across our 75 different images. Since all 784 features are connected through their own “synapse” to all three hidden neurons, the weights between the input layer and the hidden layer can be represented as a 3x784 matrix.</p>

<p><em>To follow along from here, make sure that you are familiar with <a href="https://www.youtube.com/embed/kT4Mp9EdVqs">matrix multiplication</a>.</em></p>

<p>Using a single matrix multiplication, we can represent the multiplication of all 784x75 features with their corresponding weight <em>as well as</em> each hidden neuron’s summation of its weighted input. With the columns representing a single sample, our input matrix €X€ looks like this:</p>

<p>€€
\begin{bmatrix}
x_{1,1} &amp; x_{1,2} &amp; \cdots &amp; x_{1,75}
\cr\cr
\vdots &amp; \vdots &amp; \vdots &amp; \vdots
\cr\cr
x_{784,1} &amp; x_{784,2} &amp; \cdots &amp; x_{784,75}
\end{bmatrix}
€€</p>

<p>In our first weight matrix €\Theta^{(1)}€, the rows represent the weights that lead to each particular hidden neuron. With three hidden neurons, this gives us a 3x784 matrix:</p>

<p>€€
\begin{bmatrix}
\theta_{1,1} &amp; \theta_{1,2} &amp; \cdots &amp; \theta_{1,784}
\cr\cr
\theta_{2,1} &amp; \theta_{2,2} &amp; \cdots &amp; \theta_{2,784}
\cr\cr
\theta_{3,1} &amp; \theta_{3,2} &amp; \cdots &amp; \theta_{3,784}
\end{bmatrix}
€€</p>

<p>When we multiply €X€ and €\Theta^{(1)}€, we get the matrix below. (Note that subscript €*€ either signifies entire rows or entire columns, so that €\theta_{i *}€ means “the entire €i€th row of €\Theta€”, and €\theta_{* j}€ means “the entire €j€th column of €\Theta€”.)</p>

<p>€€
\Theta^{(1)}X=\begin{bmatrix}
\theta_{1 *}x_{* 1} &amp; \theta_{1 *}x_{* 2} &amp; \cdots &amp; \theta_{1 *}x_{* 75}
\cr\cr
\theta_{2 *}x_{* 1} &amp; \theta_{2 *}x_{* 2} &amp; \cdots &amp; \theta_{2 *}x_{* 75}
\cr\cr
\theta_{3 *}x_{* 1} &amp; \theta_{3 *}x_{* 2} &amp; \cdots &amp; \theta_{3 *}x_{* 75}
\end{bmatrix}
€€</p>

<p>Our matrix of samples €X€ contains 75 column vectors €x_{*j}€, while our matrix of weights €\Theta^{(1)}€ contains 3 row vectors €\theta_{i*}€. When we multiply these, every column vector (sample) in €X€ is <a href="https://www.youtube.com/watch?v=WNuIhXo39_k&amp;t=20s">dotted</a> with every row vector in €\Theta^{(1)}€. This yields a new 3x75 matrix that contains the sum of the weighted inputs for all three hidden neurons and <em>for all 75 samples</em>.</p>

<p>An easy way to remember the dynamics of matrix multiplication is that a matrix of dimensions <strong>AxB</strong> multiplied with a matrix of dimensions <strong>BxD</strong> results in a matrix of dimensions <strong>AxD</strong>. Thus, when we multiply a 3x784 matrix with a 784x75 matrix, we get a 3x75 matrix.</p>

<p>For practical reasons that we will see later, our biases are kept in their own self-contained vector. Since each hidden neuron has its own bias, the bias vector for the hidden layer €b^{(1)}€ is three-dimensional. To proceed with the forward pass, we add each bias to the sum of the weighted input for its corresponding hidden neuron. This yields a new matrix €Z^{(1)}€.</p>

<p>Now that we have a matrix containing the sum of the weighted inputs for all three hidden neurons and for all 75 samples, we need to pass it through a nonlinear activation function (we will see why this is later). Just as in <a href="http://sigmoidprime.com/post/how-to-build-a-neural-network-pt-2/">Pt. 2</a>, we will be using the logistic sigmoid function:</p>

<p>€€
\sigma(z) = \frac{1}{1+\exp(-z)}
€€</p>

<p>Once we apply the sigmoid function element-wise to the sums of the weighted inputs, we have the full set of activations from the hidden layer in the form of a matrix €A€. Like €Z^{(1)}€, €A€ has dimensions 3x75. In order to send the activations on through the network, they need to be multiplied with the second set of weights €\Theta^{(2)}€ (this is the one between the hidden layer and the output layer). Because we have three hidden neurons and 10 output neurons, the dimensions of the second weight matrix will be 10x3.</p>

<p>Can you guess what happens next? We simply take our 3x75 matrix of activations from the hidden layer and multiply it by the second weight matrix €\Theta^{(2)}€. This gives us a new 10x75 matrix. Adding the 10-dimensional bias vector €b^{(2)}€ to each of the 75 columns yields a matrix containing the sums of the weighted inputs €Z^{(2)}€ for the output layer.</p>

<p>At this point, we are almost finished. Now we just need to convert the sums of the weighted inputs to the output layer €Z^{(2)}€ into the actual output of the network €\hat{Y}€. It might be tempting to apply the sigmoid function again, but doing so would end up posing a problem.</p>

<p>You may remember from previously that we interpreted the output of the sigmoid function as the probability of a sample belonging to one of two classes. In a neural network with multiple output neurons however, if we apply the sigmoid function at the output layer, the output of each neuron will be independent from all the others. This would have the undesirable consequence that we would be unable to interpret the output of the network as a probability distribution.</p>

<p><img src="/assets/posts/3/wrong_output_2.png" width="250" /></p>

<p>To avoid this problem, we will instead be using a function known as the <strong>softmax function</strong>. The softmax function will <em>only</em> be applied at the output layer, and will be denoted as €\delta€.</p>

<p>€€
\hat{y}_{i,j} = \delta({z_{i, j}})
=
\frac{\exp({z_{i,j})}}{\sum\limits_{k=1}^{n}\exp(z_{k,j})}
€€</p>

<p>Here, €z_{i, j}€ is the summed input to output neuron €i€ for sample €j€. With 10 output neurons and 75 samples, €i€ would thus range from 1 to 10, and €j€ from 1 to 75. €n€ is the number of outputs for each sample, and is therefore equal to the number of output neurons.</p>

<p>Can you tell what the function does by looking at the expression?</p>

<p>By placing €\exp(z_{i,j})€ in the numerator and €\sum\limits_{k=1}^{n}\exp(z_{k,j})€ in the denominator, we ensure that the total output of the network always sums to 1. Now, when the network outputs vectors of numbers, we can interpret the output as a probability distribution. This allows us to then choose to classify the sample as belonging to whatever class the output neuron with the highest probability represents.</p>

<p><img src="/assets/posts/3/right_output_2.png" width="240" /></p>

<p>This marks the end of the forward pass. Now to some intuition!</p>

<h2 id="what-is-actually-happening-here">What is actually happening here?</h2>

<p>Now that we have seen <em>what</em> a neural network does, it is time to answer the question of <em>why</em> it works. <a href="http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/">As explained by Chris Olah</a>, the process can be separated into three distinct steps: <strong>linear transformations</strong>, <strong>translations</strong>, and applications of a <strong>nonlinearity</strong>.</p>

<p><strong>Linear transformations and translations</strong>: A linear transformation scales, skews, or rotates a coordinate space by redefining the directions and amplitudes of its <strong>basis vectors</strong>. In the case of a 2D coordinate space, the only two basis vectors  — €\bar{x}€ and €\bar{y}€ — are defined as €\begin{bmatrix}1 \cr 0 \end{bmatrix}€ and €\begin{bmatrix}0 \cr 1 \end{bmatrix}€ respectively. While they may not look like much, €\bar{x}€ and €\bar{y}€ fully define their own coordinate space. To see why, let us first join them into a matrix:</p>

<p>€€
\begin{bmatrix}1 &amp; 0 \cr 0 &amp; 1 \end{bmatrix}
€€</p>

<p>Then, let us take an arbitrary point €\begin{bmatrix}x \cr y \end{bmatrix}€ and multiply it by our matrix:</p>

<p>€€
\begin{bmatrix}1 &amp; 0 \cr 0 &amp; 1 \end{bmatrix} \begin{bmatrix}x \cr y \end{bmatrix} = \begin{bmatrix}1x+0y \cr 0x+1y \end{bmatrix} = \begin{bmatrix}x \cr y \end{bmatrix}
€€</p>

<p>Using matrix-vector multiplication, we just showed that in a coordinate space with these basis vectors, the point €\begin{bmatrix}x \cr y \end{bmatrix}€ remains unchanged.</p>

<p>Now let’s see what happens when we double €\bar{x}€:</p>

<p>€€
\begin{bmatrix}2 &amp; 0 \cr 0 &amp; 1 \end{bmatrix} \begin{bmatrix}x \cr y \end{bmatrix} = \begin{bmatrix}2x+0y \cr 0x+1y \end{bmatrix} = \begin{bmatrix}2x \cr y \end{bmatrix}
€€</p>

<p>Here, the original point €\begin{bmatrix}x \cr y \end{bmatrix}€ gets moved to €\begin{bmatrix}2x \cr y \end{bmatrix}€ when we apply the transformation. Since this is true of any and all points, we can visualize this as a transformation of the space itself:</p>

<p><img src="/assets/posts/3/animated.gif" width="800" /></p>

<p>Pretty straightforward, right? Now, let’s do something a little more interesting. Instead of merely shrinking or stretching the axes, let’s try to make them codependent. For instance, we could define a transformation like this:</p>

<p>€€
\begin{bmatrix}2 &amp; 0 \cr 1 &amp; 1 \end{bmatrix} \begin{bmatrix}x \cr y \end{bmatrix} = \begin{bmatrix}2x+0y \cr 1x+1y \end{bmatrix} = \begin{bmatrix}2x \cr x+y \end{bmatrix}
€€</p>

<p>This transformation — known as a <a href="https://en.wikipedia.org/wiki/Shear_mapping">shear</a> — has the effect that every time we increase €x€ by 1, we also increase €y€ by 1. Let’s visualize this to gain some intuition:</p>

<p><img src="/assets/posts/3/animated2.gif" width="500" /></p>

<p>Now it is easier to see what is happening. As soon as we begin increasing €x€, not only are we moving to the right, but also upwards. In this transformed space, it is impossible to change €x€ without also changing €y€. A change in €y€, however, does not affect €x€, since the <em>y</em>-axis remains orthogonal to the original <em>x</em>-axis.</p>

<p><strong>When we pass data into a neural network, this kind of transformation is what happens during the weight multiplication.</strong> One thing to note, however, is that the number of hidden neurons in each layer might be smaller or larger than the number of neurons in the layer that preceded it. This results in the dimensionality of the data either getting reduced or expanded as it passes through the network. If there are more neurons in the first hidden layer than in the input layer, this causes the dimensionality of the data to be expanded. Likewise, if there are less hidden neurons than input neurons, the dimensionality will be reduced.</p>

<p>Adding a unique bias to each hidden or output neuron has the effect of <strong>translating</strong> the space; that is, pushing or pulling our data in different directions for each dimension. Again, let’s see what this looks like:</p>

<p><img src="/assets/posts/3/translation.gif" width="500" /></p>

<p>Here, we first transform the space, and then translate it by €\begin{bmatrix}3 \cr 2 \end{bmatrix}€. Note that unless we keep the original space in the background, translations can be hard to visualize while properly keeping the space in view. I will therefore ignore them in future visualizations.</p>

<p>Finally — and perhaps most significantly — is the effect of applying a nonlinear activation function (like the logistic sigmoid function). This takes all the points of the space and applies the function to each of the point’s components individually.</p>

<p>The “nonlinear” part is important here. As opposed to linear transformations like the ones we just saw, nonlinear transformations are able to warp space in very dynamic and elastic ways. As shown below, this can make for some quite beautiful visualizations.</p>

<div style="width:inherit;height:auto;display:block;">
<img src="/assets/posts/3/nonlinear_1.gif" style="width:25%;float:left;" />
<img src="/assets/posts/3/nonlinear_2.gif" style="width:25%;float:left;margin-left:12.5%;" />
<img src="/assets/posts/3/nonlinear_3.gif" style="width:25%;float:right;" />
<div style="clear:both;"></div>
<img src="/assets/posts/3/nonlinear_4.gif" style="width:25%;float:left;" />
<img src="/assets/posts/3/nonlinear_5.gif" style="width:25%;float:left;margin-left:12.5%;" />
<img src="/assets/posts/3/nonlinear_6.gif" style="width:25%;float:right;" />
<div style="clear:both;"></div>
</div>

<p>As you can see, the shape of a space transformed by a sigmoid function is highly dependent on the space to which it is applied. This is why the weights and biases are so important; they do the groundwork that lets the activation function bend the data in the most optimal way.</p>

<p>But what is “the most optimal way?” To answer this question, we need to turn our eyes to the output layer. As we touched on in <a href="http://sigmoidprime.com/post/how-to-build-a-neural-network-pt-1/">Pt. 1</a>, the equation for a line, a plane, or a hyperplane can be expressed as the variables (€{x_1, \cdots, x_n}€) that, when multiplied by their corresponding coefficient (€{y_1, \cdots, y_n}€), sums to 0:</p>

<p>€€
x_1y_1 + x_2y_2 + \cdots + x_ny_n=0
€€</p>

<p>We also saw how the exact same thing could be expressed as the dot product between vectors:</p>

<p>€€
x \cdot y = 0
€€</p>

<p>When we have a set of activations that pass from the final hidden layer of a neural net to the output layer, we are essentially taking the vector dot product between the activations and the set of weights that lead to a particular output neuron. Each neuron then runs its input through the softmax function and interprets the output as a probability distribution over the (mutually exclusive) classes that the neurons represent.</p>

<p>Sounds familiar to logistic regression? That’s because it is. In fact, using the softmax function to generate probability distributions is also known as <strong>multinomial logistic regression</strong> in that it is used in non-binary settings.</p>

<p>This tells us something really interesting: In order for the output layer to distinguish between different classes, it needs to be able to separate them with a straight line just as in Pt. 1 and Pt. 2. However, where the perceptron and logistic regression algorithms could only handle data that was linearly separable from the get-go, a neural net takes its input data and transforms it into becoming linearly separable if it isn’t already!</p>

<p><img src="/assets/posts/3/nn_visualization.gif" width="600" /></p>

<p>This is the same dataset as the one we used in the previous post. Unlike our logistic regression algorithm however, a neural net is able to perfectly distinguish between the classes because the output layer only sees the space in which they are linearly separable.</p>

<h2 id="the-code">The code</h2>

<p>A thing that surprised me when I set up my first neural network was the brevity of the actual program. While the underlying concepts can sometimes be a challenge to fully grasp, the number of lines of code required to build one is actually very modest.</p>

<p>Let’s begin by setting up a <code class="highlighter-rouge">NeuralNetwork</code> class. We will use the <code class="highlighter-rouge">__init__</code> method to instantiate our hyperparameters:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="k">class</span> <span class="nc">NeuralNetwork</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer_sizes</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">NeuralNetwork</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>

        <span class="n">n_layers</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">)</span>

        <span class="k">if</span> <span class="p">(</span><span class="n">n_layers</span> <span class="o">&gt;=</span> <span class="mi">3</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">thetas</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="p">[]</span>

            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span> <span class="c"># a network with n layers has n-1 set of weights and biases</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">thetas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="n">layer_sizes</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]))</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">thetas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">thetas</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">"The minimum number of layers is 3 (</span><span class="si">%</span><span class="s">i provided)"</span> <span class="o">%</span> <span class="n">n_layers</span><span class="p">)</span></code></pre></figure>

<p>Our <code class="highlighter-rouge">__init__</code> method takes two parameters: <code class="highlighter-rouge">layer_sizes</code> and <code class="highlighter-rouge">alpha</code>. <code class="highlighter-rouge">layer_sizes</code> will be a list of numbers specifying the size (number of neurons) of each layer in the network. Therefore, it will also implicitly define the size of the network itself. As to the <code class="highlighter-rouge">alpha</code> parameter, there is no need to worry about that just yet; we will explore what it means in the next post.</p>

<p>To implement the forward pass, we will add a <code class="highlighter-rouge">forward_pass</code> method to <code class="highlighter-rouge">NeuralNetwork</code>:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">forward_pass</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">A</span><span class="p">):</span>
    <span class="n">Zs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">As</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">thetas</span><span class="p">)):</span>
        <span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">thetas</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">A</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">Zs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">thetas</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="n">A</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>
            <span class="n">As</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">Y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">Zs</span><span class="p">,</span> <span class="n">As</span><span class="p">,</span> <span class="n">Y_hat</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Z</span><span class="p">):</span> <span class="k">return</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">Z</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Z</span><span class="p">):</span> <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">Z</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span><span class="bp">None</span><span class="p">]</span></code></pre></figure>

<p>Here, <code class="highlighter-rouge">forward_pass</code> takes the activations of the first layer <code class="highlighter-rouge">A</code> (aka. the raw input), and runs it through the network. It completes by returning the following:</p>

<ul>
  <li>A list <code class="highlighter-rouge">Zs</code> containing matrices of weighted inputs for the hidden layers and the output layer.</li>
  <li>A list <code class="highlighter-rouge">As</code> containing matrices of activations for each hidden layer.</li>
  <li>A matrix <code class="highlighter-rouge">Y_hat</code> containing the probability distribution over all possible classes and for all samples in <code class="highlighter-rouge">A</code>.</li>
</ul>

<p>This concludes our implementation of the forward pass for our neural network. In the next (and final) post of the series, we will explore how our network can automatically find the right values for our weights and biases using <strong>backpropagation</strong>.</p>


        </article>
        <br /><br />
        <footer>
          <div class="inner-footer">
            <p><b>Questions? Comments? Corrections?</b></p>
            <br />
            <p>Whatever it may be, don't hesitate to let me know. You can get in touch on <a href="http://twitter.com/wster" target="_blank">Twitter</a> or via <a href="mailto:august.wester@gmail.com">email</a>.</p>
          </div>
        </footer>
      </div>
    </main>
    <script type="text/javascript">
      var config = [{left:"€€", right:"€€", display:true}, {left: "€", right:"€", display:false}];
      renderMathInElement(document.body, {delimiters: config});
    </script>
    
    <script>
      (function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,"script","https://www.google-analytics.com/analytics.js","ga");
      ga("create", "UA-89251189-1", "auto");
      ga("send", "pageview");
    </script>
    
  </body>
</html>
