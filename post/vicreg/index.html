<!doctype html> <html lang="en"> <head> <meta charset="utf-8"> <title>Learning Self-Supervised Image Representations With VICReg</title> <meta name="description" content=""> <meta name="twitter:card" content="summary_large_image"> <meta name="twitter:creator" content="@augustwester"> <meta name="twitter:title" content="Learning Self-Supervised Image Representations With VICReg"> <meta name="twitter:image" content="https://sigmoidprime.com/assets/posts/vicreg/thumb.png"> <meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover"> <link rel="stylesheet" href="/css/style.css"> <link rel="stylesheet" href="/libs/highlight/styles/github.min.css"> <link rel="stylesheet" href="/libs/katex/katex.min.css"> <link rel="stylesheet" href="/libs/c3/c3.min.css"> <link rel="apple-touch-icon" sizes="180x180" href="/assets/favicons/apple-touch-icon.png"> <link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"> <link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"> <link rel="manifest" href="/assets/favicons/site.webmanifest"> <link rel="mask-icon" href="/assets/favicons/safari-pinned-tab.svg" color="#d249aa"> <link rel="shortcut icon" href="/assets/favicons/favicon.ico"> <meta name="msapplication-TileColor" content="#2b5797"> <meta name="msapplication-config" content="/assets/favicons/browserconfig.xml"> <meta name="theme-color" content="#ffffff"> <link rel="preconnect" href="https://fonts.googleapis.com"> <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> <link href="https://fonts.googleapis.com/css2?family=Open+Sans&display=swap" rel="stylesheet"> <script src="/libs/highlight/highlight.pack.js"></script> <script src="/libs/katex/katex.min.js"></script> <script src="/libs/katex/contrib/auto-render.js"></script> <script>hljs.initHighlightingOnLoad();</script> </head> <body> <div id="sidebar"> <div id="dismiss-sidebar-icon-container" onclick="toggleSidebar()"> <img src="/assets/icons/cross.svg" id="dismiss-sidebar-icon" alt="Dismiss icon"> </div> <div class="picture"></div> <p class="hi">Hi. I'm <span class="highlight">August</span>.</p> <p class="bio">I'm a <span class="highlight">deep learning</span> enthusiast with a broad interest in ML <span class="highlight">research</span> and <span class="highlight">engineering</span>.</p> <p class="bio">I like to understand and explain challenging concepts in the scientific literature, and I enjoy translating abstract theoretical ideas into code. This blog is an excuse for me to do both.</p> <p class="bio">Feel free to reach out if you'd like to say hi üòä</p> <div class="social-media-container"> <div class="social-media-icons"> <a href="https://twitter.com/augustwester"><img src="/assets/icons/twitter.svg" style="width:35px;height:28px;" alt="Twitter icon"></a> <a href="https://github.com/augustwester"><img src="/assets/icons/github.svg" style="width:28px;height:28px;" alt="GitHub icon"></a> <a href="mailto:august.wester@gmail.com"><img src="/assets/icons/email.svg" style="width:28px;height:28px;" alt="Email icon"></a> </div> </div> </div> <div id="overlay"> <div id="content"> <header class="top-header"> <div id="hamburger" onclick="toggleSidebar()"> <div class="sidebar-icon-top"></div> <div class="sidebar-icon-bottom"></div> </div> <div class="inner-header"> <div class="logo-container"> <a href="/"><span class="logo">sigmoid<span class="prime">'</span></span></a> <span class="tagline">a machine learning blog</span> </div> </div> </header> <main style="margin-top:80px;"> <div class="article-container "> <article> <header> <h1 class="post-title">Learning Self-Supervised Image Representations With VICReg</h1> </header> <div class="date-ct"> <div class="date-span"> <p>March 3, 2023</p> </div> <div class="github-span"> <img src="/assets/icons/github.svg"> <a href="https://github.com/augustwester/vicreg" target="_blank">Code available on GitHub</a> </div> <div style="clear:both"></div> </div> <p>How can we learn good image representations without the need for labeled datasets? Research in this area holds the promise of providing to the vision domain the same abundance of training data that has fueled progress in NLP. In this post, we will explore the problem of SSL through the lens of VICReg<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup>, a recent proposal for self-supervised learning of image representations proposed by researchers at FAIR and published at ICLR 2022. We will implement our own version of the model and train it on CIFAR-10, making it feasible to run on a single GPU.</p> <h2 id="ssl-in-the-vision-domain">SSL in the vision domain</h2> <p>The most prominent use of SSL today is the training of LLMs. Here, a sequence of token embeddings is causally masked, and the model is trained to predict token $t$ given tokens $&lt;t$. In vision, the closest analogue to this approach is to incorporate a temporal dimension (in the form of successive video frames) and training a model to predict frame $t$ given frames $&lt;t$.</p> <p>However, as <a href="https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/">Yann LeCun and Ishan Misra explain</a>, such an approach is currently infeasible. This is due to the fact that the continuous and high-dimensional nature of images makes it much harder to represent uncertainty compared with the discrete and relatively low-dimensional nature of text, where a vocabulary can be easily vectorized and optimized using cross entropy.</p> <p>Today, the most popular approaches to SSL in the vision domain instead rely on <em>joint embedding architectures</em>, where two different augmentations of the same image are input to separate branches of a model, each consisting of an <em>encoder</em> and a <em>projector</em>. Each branch outputs an embedding, and the model is trained to minimize the distance between pairs of embeddings corresponding to the same image. When the same architecture is used in both branches and share weights, this is known as a ‚Äúsiamese‚Äù architecture.</p> <p><img src="/assets/posts/vicreg/siamese-architecture.png" width="100%" /></p> <p class="subtext">A joint embedding architecture is trained to produce similar embeddings for different augmentations of the same image.</p> <p>The purpose of the encoder is to learn useful representations for downstream tasks such as image classification, while the projector is responsible for eliminating the information by which the two representations differ. Once the model has been trained, the projector is discarded<sup id="fnref:2"><a href="#fn:2" class="footnote">2</a></sup>, and the encoder can be fine-tuned for specific problems like image classification using much less labeled data than what is usually required when training in a purely supervised fashion.</p> <h2 id="collapse">Collapse</h2> <p>While this captures the basic idea behind self-supervised pre-training, it is not the whole story. If we blindly follow this approach, we will quickly discover that our models end up ignoring their inputs by <em>collapsing</em> their embeddings onto the zero-vector, thus minimizing the distance.</p> <p>Broadly speaking, there are two approaches to combat the problem of collapse:</p> <ol> <li><strong>Contrastive</strong> approaches like SimCLR<sup id="fnref:3"><a href="#fn:3" class="footnote">3</a></sup> not only minimize the distance between embeddings corresponding to the same image (i.e. positive pairs), but also maximize the distance between embeddings corresponding to different images (i.e. negative pairs). By enforcing dissimilarity between negative pairs, collapsed embeddings become suboptimal.</li> <li><strong>Non-contrastive</strong> approaches dispense with negative pairs but currently do so in favor of poorly understood optimization tricks. For instance, BYOL<sup id="fnref:4"><a href="#fn:4" class="footnote">4</a></sup> uses a so-called <em>momentum encoder</em>, which anchors the parameters of one branch to an exponential moving average of the parameters of the other. More recently, SimSiam<sup id="fnref:5"><a href="#fn:5" class="footnote">5</a></sup> showed how the momentum encoder can be discarded by introducing an alternating stop-gradient operator for each branch.</li> </ol> <p>Contrastive methods tend to require large numbers of negative pairs to learn meaningful representations. In the case of SimCLR, which relies on in-batch sampling of negative pairs, this has the unfortunate side-effect of increasing the batch size to thousands of images. Meanwhile, reliance on a momentum encoder means that a non-contrastive model like BYOL requires identical architectures in each branch. This makes the model unsuitable for multi-modal SSL tasks like those used in training CLIP<sup id="fnref:6"><a href="#fn:6" class="footnote">6</a></sup>.</p> <h2 id="vicreg">VICReg</h2> <p>Proposed by Bardes et al., VICReg (short for Variance-Invariance-Covariance Regularization) is a more recent approach to self-supervised pre-training using a joint embedding architecture. It alleviates many of the problems with previous methods, such as reliance on siamese networks, large batch sizes, and poorly understood optimization tricks. Instead, it proposes a simple and model-agnostic training procedure with an intuitive interpretation and performance on-par with other methods (75.5% Top-1 accuracy on ImageNet with a siamese ResNet-50 backbone).</p> <p><img src="/assets/posts/vicreg/loss.png" width="65%" /></p> <p>The novelty of VICReg lies in its loss function which consists of a <strong>variance</strong> term that repels negative pairs, an <strong>invariance</strong> term that attracts positive pairs, and a <strong>covariance</strong> term that prevents the individual components of each embedding from encoding similar information (think $\beta$-VAE<sup id="fnref:7"><a href="#fn:7" class="footnote">7</a></sup> and disentangled latent factors). Let‚Äôs go through each of these one by one.</p> <p>The <strong>variance</strong> term $v(Z)$ below is defined for a batch $Z \in \mathbb{R}^{n \times d}$ of $n$ embeddings:</p> <p>‚Ç¨‚Ç¨ v(Z) = \frac{1}{d} \sum_{j=1}^d \max(0, \gamma - \sqrt{\text{Var}(z^j) + \epsilon}), ‚Ç¨‚Ç¨</p> <p>Here, $z^j \in \mathbb{R}^n$ is the vector comprising the $j$‚Äôth element of all $n$ embeddings and $\gamma$ is a target standard deviation, which the authors set to $1$. Minimizing this expression drives the batch-wise standard deviation of each embedding dimension towards $\gamma$, thus encouraging diversity between negative pairs and preventing collapse.</p> <p>Meanwhile, the <strong>invariance</strong> term below attracts positive pairs by minimizing the mean-squared Euclidean distance between embeddings $Z$ and $Z‚Äô$ of each branch:</p> <p>‚Ç¨‚Ç¨ s(Z, Z‚Äô) = \frac{1}{n} \sum_{i=1}^n ||z_i - z_i‚Äô||^2_2 ‚Ç¨‚Ç¨</p> <p>The <strong>covariance</strong> term for a batch $Z$ is computed by first obtaining the covariance matrix $C(Z)$:</p> <p>‚Ç¨‚Ç¨ C(Z) = \frac{1}{n-1} \sum_{i=1}^n (z_i - \mu)(z_i - \mu)^T, ‚Ç¨‚Ç¨</p> <p>where $\mu$ is the batch-wise mean vector. The covariance loss $c(Z)$ is then defined as</p> <p>‚Ç¨‚Ç¨ c(Z) = \frac{1}{d} \sum_{i \neq j} [C(Z)]^2_{i,j}, ‚Ç¨‚Ç¨</p> <p>i.e. the average squared off-diagonal terms in the covariance matrix. Decorrelating the dimensions in this fashion prevents the embeddings from incorporating redundant information, thus maximizing their information content.</p> <h2 id="loss-coefficients">Loss coefficients</h2> <p>The variance term $v(Z)$, the invariance term $s(Z, Z‚Äô)$, and covariance term $c(Z)$ are gathered into the following loss function $\ell$, which is minimized during training:</p> <p>‚Ç¨‚Ç¨ \ell(Z, Z‚Äô) = \lambda s(Z, Z‚Äô) + \mu [v(Z) + v(Z‚Äô)] + \nu [c(Z) + c(Z‚Äô)] ‚Ç¨‚Ç¨</p> <p>A proper configuration of the coefficients $\lambda$, $\mu$, and $\nu$ is required for the model not to collapse its representations onto the zero-vector. This is shown in the table below.</p> <p><img src="/assets/posts/vicreg/coefficient-eval.png" width="50%" /></p> <p class="subtext">From appendix D.4 in the VICReg paper</p> <p>The authors set $\lambda=25$, $\mu=25$, and $\nu=1$, which corresponds to the best result obtained in the table. They also mention that the same values work well on other datasets such as CIFAR-10. In our implementation, we will be using the same configuration.</p> <p>It is interesting to note how much of a difference the covariance term makes in terms of downstream classification accuracy. By decorrelating the embedding dimensions, the accuracy shoots up from 57.5% to 68.6% Top-1 accuracy on ImageNet. If you are wondering why this accuracy is lower than the 75.5% mentioned above, this is due to the number of pre-training epochs used in the experiments. The results above were obtained after 100 epochs of pre-training while the 75.5% result was obtained after 1000 epochs.</p> <h2 id="results-on-imagenet">Results on ImageNet</h2> <p>In the original paper, the authors use a ResNet-50 with output dimension 2048 as the encoder. Their projector is an MLP with two hidden layers of dimension 8192 as well as an output dimension of 8192. They evaluate the model on ImageNet and achieve the results below.</p> <p><img src="/assets/posts/vicreg/imagenet-eval.png" width="85%" /></p> <p>Here, ‚ÄúLinear‚Äù refers to the linear evaluation protocol, where a linear layer is added to the (frozen) encoder and trained for classification using the full training set. The resulting accuracy then acts as a proxy measure for representation quality. ‚ÄúSemi-supervised‚Äù refers to fine-tuning the linear layer <em>and</em> the encoder using only a subset (1% or 10%) of the training data.</p> <h2 id="implementation">Implementation</h2> <p>To keep things lightweight and feasible to run on a single GPU, we will implement the model using a ResNet-18 with output dimension 512. Our projector will be an MLP with two hidden layers and dimension 1024. We will evaluate it on CIFAR-10.</p> <p>Let‚Äôs start by defining a simple <code class="highlighter-rouge">Projector</code> MLP using ReLU activations and batch normalization as prescribed in the paper:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">Projector</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder_dim</span><span class="p">,</span> <span class="n">projector_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">network</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">encoder_dim</span><span class="p">,</span> <span class="n">projector_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">projector_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">projector_dim</span><span class="p">,</span> <span class="n">projector_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">projector_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">projector_dim</span><span class="p">,</span> <span class="n">projector_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">projector_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">projector_dim</span><span class="p">,</span> <span class="n">projector_dim</span><span class="p">)</span>
        <span class="p">)</span>
            
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></code></pre></figure> <p>We‚Äôll then create a <code class="highlighter-rouge">VICReg</code> class wrapping a <code class="highlighter-rouge">torchvision.models.resnet.resnet18</code> encoder and an instance of our <code class="highlighter-rouge">Projector</code> class.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">VICReg</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder_dim</span><span class="p">,</span> <span class="n">projector_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">resnet18</span><span class="p">(</span><span class="n">num_classes</span><span class="o">=</span><span class="n">encoder_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">maxpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">projector</span> <span class="o">=</span> <span class="n">Projector</span><span class="p">(</span><span class="n">encoder_dim</span><span class="p">,</span> <span class="n">projector_dim</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">))</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">projector</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span></code></pre></figure> <p>You‚Äôll notice that we‚Äôre replacing the first convolutional layer of the ResNet encoder with a new <code class="highlighter-rouge">Conv2d</code> layer as well as replacing the max pooling layer with identity. This is because the ResNet architecture uses a $7 \times 7$ kernel in its first layer, since it is optimized for the much larger images in ImageNet. This is too reductive for the $32 \times 32$ images of CIFAR-10, so we‚Äôre replacing it with a $3 \times 3$ kernel and removing the max pooling. This is also the approach taken in the SimCLR paper when evaluating their model on CIFAR-10<sup id="fnref:8"><a href="#fn:8" class="footnote">8</a></sup>.</p> <p>We‚Äôll then create the variance, invariance, and covariance functions</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">variance</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">relu</span><span class="p">(</span><span class="n">gamma</span> <span class="o">-</span> <span class="n">z</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">invariance</span><span class="p">(</span><span class="n">z1</span><span class="p">,</span> <span class="n">z2</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">mse_loss</span><span class="p">(</span><span class="n">z1</span><span class="p">,</span> <span class="n">z2</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">covariance</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="n">n</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">cov</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">"ni,nj-&gt;ij"</span><span class="p">,</span> <span class="n">z</span><span class="o">-</span><span class="n">mu</span><span class="p">,</span> <span class="n">z</span><span class="o">-</span><span class="n">mu</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">off_diag</span> <span class="o">=</span> <span class="n">cov</span><span class="o">.</span><span class="nb">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span> <span class="o">-</span> <span class="n">cov</span><span class="o">.</span><span class="nb">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">diag</span><span class="p">()</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">off_diag</span> <span class="o">/</span> <span class="n">d</span></code></pre></figure> <p>and next the randomized data augmentations used in training:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">Augmentation</span><span class="p">:</span>
    <span class="n">augment</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">RandomResizedCrop</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">RandomHorizontalFlip</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">RandomApply</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ColorJitter</span><span class="p">(</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)],</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.8</span><span class="p">),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">RandomGrayscale</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">RandomApply</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">GaussianBlur</span><span class="p">(</span><span class="mi">3</span><span class="p">)],</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">RandomSolarize</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.4914</span><span class="p">,</span> <span class="mf">0.4822</span><span class="p">,</span> <span class="mf">0.4465</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.247</span><span class="p">,</span> <span class="mf">0.243</span><span class="p">,</span> <span class="mf">0.261</span><span class="p">))</span>
    <span class="p">])</span>
        
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">augment</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">augment</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></code></pre></figure> <p>We‚Äôre wrapping the sequence of transformations in a class <code class="highlighter-rouge">Augmentation</code>, since this allows us to override <code class="highlighter-rouge">__call__</code> and return two different augmentations for every input. By supplying an instance of <code class="highlighter-rouge">Augmentation</code> as the transform when specifying the dataset, our <code class="highlighter-rouge">DataLoader</code> will iterate over <em>tuples</em> of differently augmented batches, i.e.:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">data</span> <span class="o">=</span> <span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s">"."</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">Augmentation</span><span class="p">())</span>
<span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>

<span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
	<span class="n">x1</span> <span class="o">=</span> <span class="n">images</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c"># this is the batch augmented in one way</span>
	<span class="n">x2</span> <span class="o">=</span> <span class="n">images</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c"># this is the same batch augmented in a different way</span></code></pre></figure> <p>We can now train the model:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">)):</span>
    <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
        <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">images</span>
        <span class="n">z1</span><span class="p">,</span> <span class="n">z2</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
        
        <span class="n">la</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">nu</span> <span class="o">=</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">1</span>
        <span class="n">var1</span><span class="p">,</span> <span class="n">var2</span> <span class="o">=</span> <span class="n">variance</span><span class="p">(</span><span class="n">z1</span><span class="p">),</span> <span class="n">variance</span><span class="p">(</span><span class="n">z2</span><span class="p">)</span>
        <span class="n">inv</span> <span class="o">=</span> <span class="n">invariance</span><span class="p">(</span><span class="n">z1</span><span class="p">,</span> <span class="n">z2</span><span class="p">)</span>
        <span class="n">cov1</span><span class="p">,</span> <span class="n">cov2</span> <span class="o">=</span> <span class="n">covariance</span><span class="p">(</span><span class="n">z1</span><span class="p">),</span> <span class="n">covariance</span><span class="p">(</span><span class="n">z2</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">la</span><span class="o">*</span><span class="n">inv</span> <span class="o">+</span> <span class="n">mu</span><span class="o">*</span><span class="p">(</span><span class="n">var1</span> <span class="o">+</span> <span class="n">var2</span><span class="p">)</span> <span class="o">+</span> <span class="n">nu</span><span class="o">*</span><span class="p">(</span><span class="n">cov1</span> <span class="o">+</span> <span class="n">cov2</span><span class="p">)</span>
        
        <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span></code></pre></figure> <p>In the paper, the authors use the LARS optimizer<sup id="fnref:9"><a href="#fn:9" class="footnote">9</a></sup>, which is tailored for training with large batch sizes. Since VICReg doesn‚Äôt require large batches, our implementation uses the more common Adam optimizer with learning rate $2 \cdot 10^{-4}$, default momentum parameters and a weight decay of $10^{-6}$. I encourage you to explore different optimizers and see how it affects performance (although this might be quite time-consuming depending on your hardware).</p> <p>In my experience, the model keeps improving after 500 epochs of training with batch size 256, although slowly. At 500 epochs, the model achieves 87% accuracy on the CIFAR-10 test set in linear evaluation (see <code class="highlighter-rouge">eval.py</code> on <a href="https://github.com/augustwester/vicreg">GitHub</a>). I suspect you can reach close to 90% if you let it train for at least another few hundred epochs. You can download the 500-epoch checkpoint <a href="https://sigmoidprime.s3.eu-central-1.amazonaws.com/vicreg/checkpoint.pt">here</a> if you are interested in trying this.</p> <h2 id="conclusion">Conclusion</h2> <p>The merits of self-supervised learning are obvious but the field is still figuring out how best to apply SSL methods outside of NLP. It‚Äôs conceivable that computer vision, too, will come to be dominated by large models pre-trained on massive quantities of images and video and fine-tuned for downstream tasks like classification or segmentation. Will computer vision have a ‚ÄúGPT moment‚Äù when pre-training is done not on the ~1M images in ImageNet but 100 billion images scraped from the web, and what will the results be?</p> <h2 id="notes-and-references">Notes and references</h2> <div class="footnotes"> <ol> <li id="fn:1"> <p>Bardes, Adrien and Ponce, Jean and LeCun, Yann (2021). <a href="https://arxiv.org/abs/2105.04906">VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning</a>¬†<a href="#fnref:1" class="reversefootnote">&#8617;</a></p> </li> <li id="fn:2"> <p>Since the purpose of the projector is primarily reductive (i.e. to produce embeddings that are invariant to data transformations by filtering out information such as color or orientation), the encoder representations contain more information and are thus more useful for downstream tasks. For more on this, see section 4.2 of the <a href="https://arxiv.org/abs/2002.05709">SimCLR paper</a>.¬†<a href="#fnref:2" class="reversefootnote">&#8617;</a></p> </li> <li id="fn:3"> <p>Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey (2020). <a href="https://arxiv.org/abs/2002.05709">A Simple Framework for Contrastive Learning of Visual Representations</a>¬†<a href="#fnref:3" class="reversefootnote">&#8617;</a></p> </li> <li id="fn:4"> <p>Grill, Jean-Bastien and Strub, Florian and Altch√©, Florent and Tallec, Corentin and Richemond, Pierre H. and Buchatskaya, Elena and Doersch, Carl and Pires, Bernardo Avila and Guo, Zhaohan Daniel and Azar, Mohammad Gheshlaghi and Piot, Bilal and Kavukcuoglu, Koray and Munos, R√©mi and Valko, Michal (2020). <a href="https://arxiv.org/abs/2006.07733">Bootstrap your own latent: A new approach to self-supervised Learning</a>¬†<a href="#fnref:4" class="reversefootnote">&#8617;</a></p> </li> <li id="fn:5"> <p>Chen, Xinlei and He, Kaiming (2020). <a href="https://arxiv.org/abs/2011.10566">Exploring Simple Siamese Representation Learning</a>¬†<a href="#fnref:5" class="reversefootnote">&#8617;</a></p> </li> <li id="fn:6"> <p>Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya (2021). <a href="https://arxiv.org/abs/2103.00020">Learning Transferable Visual Models From Natural Language Supervision</a>¬†<a href="#fnref:6" class="reversefootnote">&#8617;</a></p> </li> <li id="fn:7"> <p>Irina Higgins and Loic Matthey and Arka Pal and Christopher Burgess and Xavier Glorot and Matthew Botvinick and Shakir Mohamed and Alexander Lerchner (2017). <a href="https://openreview.net/forum?id=Sy2fzU9gl">beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework</a>¬†<a href="#fnref:7" class="reversefootnote">&#8617;</a></p> </li> <li id="fn:8"> <p>See Appendix B.9 of Chen et al. (2020).¬†<a href="#fnref:8" class="reversefootnote">&#8617;</a></p> </li> <li id="fn:9"> <p>You, Yang and Gitman, Igor and Ginsburg, Boris (2017). <a href="https://arxiv.org/abs/1708.03888">Large Batch Training of Convolutional Networks</a>¬†<a href="#fnref:9" class="reversefootnote">&#8617;</a></p> </li> </ol> </div> </article> <br /><br /> <footer> <div class="inner-footer"> <p><b>Questions? Comments? Corrections?</b></p> <br /> <p>Whatever it may be, don't hesitate to let me know. You can get in touch on <a href="https://twitter.com/augustwester" target="_blank">Twitter</a> or via <a href="mailto:august.wester@gmail.com">email</a>.</p> </div> </footer> </div> </main> </div> </div> <script> function toggleSidebar() { let content = document.getElementById("content"); let sidebar = document.getElementById("sidebar"); let overlay = document.getElementById("overlay"); let hamburger = document.getElementById("hamburger"); if (content.classList.contains("translate-right")) { content.classList.remove("translate-right"); sidebar.classList.remove("translate-right"); content.style.pointerEvents = "auto"; overlay.style.cursor = "auto"; document.body.style.overflow = "visible"; overlay.onmouseup = undefined; } else { content.classList.add("translate-right"); content.style.pointerEvents = "none"; overlay.style.cursor = "pointer"; sidebar.classList.add("translate-right"); content.style.pointerEvents = "none"; document.body.style.overflow = "hidden"; overlay.onmouseup = function() { toggleSidebar(); } } } </script> <script type="text/javascript"> var config = [{left:"‚Ç¨‚Ç¨", right:"‚Ç¨‚Ç¨", display:true}, {left: "$", right:"$", display:false}]; renderMathInElement(document.body, {delimiters: config}); </script> <script> (function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","https://www.google-analytics.com/analytics.js","ga"); ga("create", "UA-89251189-1", "auto"); ga("send", "pageview"); </script> </body> </html>
