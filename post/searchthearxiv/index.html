<!doctype html> <html lang="en"> <head> <meta charset="utf-8"> <title>Building a Semantic Search Engine With OpenAI and Pinecone</title> <meta name="description" content="A walkthrough of how to build a semantic search engine for scientific papers using OpenAI and Pinecone."> <meta name="twitter:card" content="summary_large_image"> <meta name="twitter:creator" content="@augustwester"> <meta name="twitter:title" content="Building a Semantic Search Engine With OpenAI and Pinecone"> <meta name="twitter:image" content="https://sigmoidprime.com/assets/posts/searchthearxiv/thumb.png"> <meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover"> <link rel="stylesheet" href="/css/style.css"> <link rel="stylesheet" href="/libs/highlight/styles/github.min.css"> <link rel="stylesheet" href="/libs/katex/katex.min.css"> <link rel="stylesheet" href="/libs/c3/c3.min.css"> <link rel="apple-touch-icon" sizes="180x180" href="/assets/favicons/apple-touch-icon.png"> <link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"> <link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"> <link rel="manifest" href="/assets/favicons/site.webmanifest"> <link rel="mask-icon" href="/assets/favicons/safari-pinned-tab.svg" color="#d249aa"> <link rel="shortcut icon" href="/assets/favicons/favicon.ico"> <meta name="msapplication-TileColor" content="#2b5797"> <meta name="msapplication-config" content="/assets/favicons/browserconfig.xml"> <meta name="theme-color" content="#ffffff"> <link rel="preconnect" href="https://fonts.googleapis.com"> <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> <link href="https://fonts.googleapis.com/css2?family=Open+Sans&display=swap" rel="stylesheet"> <script src="/libs/highlight/highlight.pack.js"></script> <script src="/libs/katex/katex.min.js"></script> <script src="/libs/katex/contrib/auto-render.js"></script> <script>hljs.initHighlightingOnLoad();</script> </head> <body> <div id="sidebar"> <div id="dismiss-sidebar-icon-container" onclick="toggleSidebar()"> <img src="/assets/icons/cross.svg" id="dismiss-sidebar-icon" alt="Dismiss icon"> </div> <div class="picture"></div> <p class="hi">Hi. I'm <span class="highlight">August</span>.</p> <p class="bio">I'm a <span class="highlight">deep learning</span> enthusiast with broad interests in ML <span class="highlight">research</span> and <span class="highlight">engineering</span>.</p> <p class="bio">I like to understand and explain challenging concepts in the scientific literature, and I enjoy translating abstract theoretical ideas into code. This blog is an excuse for me to do both.</p> <p class="bio">Feel free to reach out if you'd like to say hi 😊</p> <div class="social-media-container"> <div class="social-media-icons"> <a href="https://twitter.com/augustwester"><img src="/assets/icons/twitter.svg" style="width:35px;height:28px;" alt="Twitter icon"></a> <a href="https://github.com/augustwester"><img src="/assets/icons/github.svg" style="width:28px;height:28px;" alt="GitHub icon"></a> <a href="mailto:august.wester@gmail.com"><img src="/assets/icons/email.svg" style="width:28px;height:28px;" alt="Email icon"></a> </div> </div> </div> <div id="overlay"> <div id="content"> <!-- <header class="top-header"> --> <div id="hamburger" onclick="toggleSidebar()"> <!--<div class="sidebar-icon-top"></div> <div class="sidebar-icon-bottom"></div>--> <div class="hamburger-bar"></div> <div class="hamburger-bar"></div> </div> <header> <div id="logo-container"> <a href="/"><span id="logo">sigmoid<span id="logo-prime">'</span></span></a> <span id="logo-tagline">a machine learning blog</span> </div> <div></div> </header> <main style="margin-top:80px;"> <div class="article-container "> <article> <header> <h1 class="post-title">Building a Semantic Search Engine With OpenAI and Pinecone</h1> </header> <div class="date-ct"> <div class="date-span"> <p>March 23, 2023</p> </div> <div class="github-span"> <img src="/assets/icons/github.svg"> <a href="https://github.com/augustwester/searchthearxiv" target="_blank">Code available on GitHub</a> </div> <div style="clear:both"></div> </div> <p>In this post, we will walk through how to build a simple semantic search engine using an OpenAI embedding model and a <a href="https://www.pinecone.io">Pinecone</a> vector database. More specifically, we will see how to build <a href="https://searchthearxiv.com">searchthearxiv.com</a>, a semantic search engine enabling students and researchers to search across more than 250,000 ML papers on arXiv using natural language. The principles covered will be general enough for you to apply the same techniques to your own dataset, so you can supercharge search across your own set of documents.</p> <h2 id="step-0-overview">Step 0: Overview</h2> <p>Before we get our hands dirty, let us first break down the problem we are trying to solve into discrete steps that we can attack one by one:</p> <ol> <li><strong>We need data.</strong> This could be in the form of raw PDF documents, an SQL database, or even a raw JSON file. In our case, we will be using <a href="https://www.kaggle.com/datasets/Cornell-University/arxiv">the arXiv metadataset on Kaggle</a>, which contains metadata (such as title and abstract) for every paper on arXiv in JSON format.</li> <li><strong>We need embeddings.</strong> Once we have our data, we need to embed every entry (in our case, every paper), such that each one is represented by a high-dimensional vector.</li> <li><strong>We need a vector index.</strong> Once we have our embeddings, we need to be able to efficiently search across them. Services like Pinecone can store millions of embeddings and allow you to do lightning-fast cosine similarity search given a query embedding (more on this later).</li> <li><strong>We need an interface.</strong> The final step is to build an interface in which users can enter queries and search our database using natural language. Typically, this would be some sort of web frontend; here we will be using a simple command-line interface instead.</li> </ol> <h2 id="step-1-data">Step 1: Data</h2> <p><a href="https://www.kaggle.com/datasets/Cornell-University/arxiv">The arXiv dataset on Kaggle</a> is maintained by Cornell University and is updated on a weekly basis. It contains every paper posted on arXiv across all STEM fields. We will be using a subset of this dataset, keeping only the ML papers. The latest version can be downloaded manually on the dataset page or, even better, using the Kaggle CLI:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kaggle datasets download -d Cornell-University/arxiv &amp;&amp; unzip arxiv.zip
</code></pre></div></div> <p>This will create a JSON file titled <code class="language-plaintext highlighter-rouge">arxiv-metadata-oai-snapshot.json</code>. The details of how we load and preprocess the data are quite boring and will hardly be applicable to your own dataset. (If you’re still curious, head over to the <a href="https://github.com/augustwester/searchthearxiv">GitHub repo</a> and inspect the code.) All you need to know is that we convert each JSON object to a custom <code class="language-plaintext highlighter-rouge">Paper</code> representation and filter out the ones published before 2012 and/or not belonging to any of the ML categories.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">JSON_FILE_PATH</span> <span class="o">=</span> <span class="s">"arxiv-metadata-oai-snapshot.json"</span>
<span class="n">CATEGORIES</span> <span class="o">=</span> <span class="p">[</span><span class="s">"cs.cv"</span><span class="p">,</span> <span class="s">"cs.lg"</span><span class="p">,</span> <span class="s">"cs.cl"</span><span class="p">,</span> <span class="s">"cs.ai"</span><span class="p">,</span> <span class="s">"cs.ne"</span><span class="p">,</span> <span class="s">"cs.ro"</span><span class="p">]</span>
<span class="n">START_YEAR</span> <span class="o">=</span> <span class="mi">2012</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Loading data..."</span><span class="p">)</span>
<span class="n">papers</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">load_data</span><span class="p">(</span><span class="n">JSON_FILE_PATH</span><span class="p">,</span> <span class="n">CATEGORIES</span><span class="p">,</span> <span class="n">START_YEAR</span><span class="p">))</span>
</code></pre></div></div> <p>The categories that we specify correspond roughly to all ML papers in the dataset and are the same used by Andrej Karpathy’s <a href="https://arxiv-sanity-lite.com/">arxiv-sanity-lite</a>.</p> <h2 id="step-2-embeddings">Step 2: Embeddings</h2> <p>The magic of modern semantic search rests on the notion of embeddings, high-dimensional vectors that encode the semantics of their underlying text.</p> <p>In order to create embeddings of the papers in our dataset, we will be using an OpenAI embedding model named <code class="language-plaintext highlighter-rouge">text-embedding-ada-002</code>. At USD 0.0004 per 1K tokens, <a href="https://openai.com/pricing">this is OpenAI’s cheapest embedding model</a>. For reference, the total number of tokens in our dataset is approximately 70 million, meaning that embedding the entire thing will cost around USD 30.</p> <p>Peeking into the <code class="language-plaintext highlighter-rouge">Paper</code> class, we find the following helper method:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="nb">property</span>
<span class="k">def</span> <span class="nf">embedding_text</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">text</span> <span class="o">=</span> <span class="p">[</span><span class="s">"Title: "</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">title</span><span class="p">,</span>
            <span class="s">"By: "</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">authors_string</span><span class="p">,</span>
            <span class="s">"From: "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">year</span><span class="p">),</span>
            <span class="s">"Abstract: "</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">abstract</span><span class="p">]</span>
    <span class="k">return</span> <span class="s">". "</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</code></pre></div></div> <p>When embedding a paper, we will thus be using an “augmented abstract” consisting of the paper’s title, the list of authors, and the year of publication. The reason we don’t embed the raw abstract is to have a chance of returning useful results for queries such as <code class="language-plaintext highlighter-rouge">paper by yoshua bengio</code> (although this type of query won’t give the most useful results in practice).</p> <p>⚠️ <strong>Danger zone:</strong> Embedding your data is not free, so be careful running the OpenAI embedding code haphazardly. It’s easy to call the API and retrieve the embeddings, only to lose them immediately once the program terminates.</p> <p>The helper function below takes a list of strings (in this case a list of “augmented abstracts”) and embeds them using the specified OpenAI model:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import openai

openai.api_key = os.environ["OPENAI_API_KEY"]

def get_embeddings(texts, model="text-embedding-ada-002"):
    embed_data = openai.Embedding.create(input=texts, model=model)
    return embed_data["data"]
</code></pre></div></div> <p>If you inspect the result of calling this function with $n$ augmented abstracts, the return value will be a list of $n$ 1536-dimensional vectors, one for each abstract. The next step is to store the embeddings in a safe place that allows for efficient search.</p> <h2 id="step-3-vector-index">Step 3: Vector index</h2> <p>To store the embeddings, we will be using Pinecone. The purpose of Pinecone is to persistently store your embeddings, while enabling you to efficiently search across them using a simple API. When you’re signed up and have created an index, you connect to it like this:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pinecone</span>

<span class="n">pinecone</span><span class="p">.</span><span class="n">init</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">"PINECONE_API_KEY"</span><span class="p">])</span>
<span class="n">index_name</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">"PINECONE_INDEX_NAME"</span><span class="p">]</span>
<span class="n">index</span> <span class="o">=</span> <span class="n">pinecone</span><span class="p">.</span><span class="n">Index</span><span class="p">(</span><span class="n">index_name</span><span class="p">)</span>
</code></pre></div></div> <p>We now create a new function that takes our list of <code class="language-plaintext highlighter-rouge">Paper</code> objects, a Pinecone index name, and the name of an OpenAI embedding model, embeds the papers and uploads them to Pinecone:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">embed_and_upsert</span><span class="p">(</span><span class="n">papers</span><span class="p">,</span> <span class="n">index_name</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">pinecone</span><span class="p">.</span><span class="n">Index</span><span class="p">(</span><span class="n">index_name</span><span class="p">,</span> <span class="n">pool_threads</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span> <span class="k">as</span> <span class="n">index</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">papers</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">)):</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="n">papers</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">batch_size</span><span class="p">]</span>
            <span class="n">texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">paper</span><span class="p">.</span><span class="n">embedding_text</span> <span class="k">for</span> <span class="n">paper</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>
            <span class="n">embed_data</span> <span class="o">=</span> <span class="n">get_embeddings</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
        
            <span class="n">pc_data</span> <span class="o">=</span> <span class="p">[(</span><span class="n">p</span><span class="p">.</span><span class="nb">id</span><span class="p">,</span> <span class="n">e</span><span class="p">[</span><span class="s">"embedding"</span><span class="p">],</span> <span class="n">p</span><span class="p">.</span><span class="n">metadata</span><span class="p">)</span>
                       <span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">embed_data</span><span class="p">)]</span>
            <span class="n">index</span><span class="p">.</span><span class="n">upsert</span><span class="p">(</span><span class="n">pc_data</span><span class="p">)</span>
</code></pre></div></div> <p>Here, <code class="language-plaintext highlighter-rouge">p.metadata</code> is a dictionary <code class="language-plaintext highlighter-rouge">{"title": paper.title, "authors": paper.authors, "abstract": paper.abstract, "year": paper.year, "month": paper.month}</code>. When fetching search results from Pinecone, this will allow us to display the paper to the user. As you will notice, each paper also has a unique id, in this case the arXiv id associated with each paper in the original dataset.</p> <p>If you call this function and keep an eye on your index in the Pinecone console, you should see the number of vectors tick up as the embeddings are uploaded.</p> <h2 id="step-4-interface">Step 4: Interface</h2> <p>The fourth and final step is to enable search across your recently created vector index. Fortunately, Pinecone makes this incredibly easy. When receiving a query, we simply embed it using the same embedding model that we used for the dataset. We then send the query embedding to Pinecone, which identifies the $k$ entries in the index with the highest cosine similarity to the query embedding (i.e. the ones that are most semantically similar).</p> <p>To see this in action, run the following script:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">openai</span>
<span class="kn">import</span> <span class="nn">pinecone</span>

<span class="n">openai</span><span class="p">.</span><span class="n">api_key</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">"OPENAI_API_KEY"</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">get_embeddings</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s">"text-embedding-ada-002"</span><span class="p">):</span>
    <span class="n">embed_data</span> <span class="o">=</span> <span class="n">openai</span><span class="p">.</span><span class="n">Embedding</span><span class="p">.</span><span class="n">create</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">texts</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">embed_data</span><span class="p">[</span><span class="s">"data"</span><span class="p">]</span>

<span class="n">pinecone</span><span class="p">.</span><span class="n">init</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">"PINECONE_API_KEY"</span><span class="p">])</span>
<span class="n">index_name</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">"PINECONE_INDEX_NAME"</span><span class="p">]</span>
<span class="n">index</span> <span class="o">=</span> <span class="n">pinecone</span><span class="p">.</span><span class="n">Index</span><span class="p">(</span><span class="n">index_name</span><span class="p">)</span>

<span class="n">query</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s">"Enter your query: "</span><span class="p">)</span>
<span class="n">embed</span> <span class="o">=</span> <span class="n">get_embeddings</span><span class="p">(</span><span class="n">query</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="s">"embedding"</span><span class="p">]</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">index</span><span class="p">.</span><span class="n">query</span><span class="p">(</span><span class="n">vector</span><span class="o">=</span><span class="n">embed</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">include_metadata</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">matches</span> <span class="o">=</span> <span class="n">response</span><span class="p">[</span><span class="s">"matches"</span><span class="p">]</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">match</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">matches</span><span class="p">):</span>
    <span class="n">metadata</span> <span class="o">=</span> <span class="n">match</span><span class="p">[</span><span class="s">"metadata"</span><span class="p">]</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">metadata</span><span class="p">[</span><span class="s">'title'</span><span class="p">]</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div> <p>This will produce an output like the following:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Enter your query: model using only attention mechanism
1: Attention Is All You Need
2: On the Dynamics of Training Attention Models
3: Focus On What's Important: Self-Attention Model for Human Pose Estimation
4: Attention-Based Models for Speech Recognition
5: Self-Attentional Models Application in Task-Oriented Dialogue Generation Systems
</code></pre></div></div> <h2 id="conclusion">Conclusion</h2> <p>Thanks to the increased availability of language models, building custom semantic search engines—once a near-impossible task for a single person—is now extremely simple to implement. While launching a site like <a href="https://searchthearxiv.com">searchthearxiv.com</a> involves additional challenges such as hosting, running a web server, keeping the database up-to-date, and so on, you should now be equipped to create your own semantic search engine using your own data.</p> <p>If you would like to dive further into the details of how <a href="https://searchthearxiv.com">searchthearxiv.com</a> works, check out the associated <a href="https://github.com/augustwester/searchthearxiv">GitHub repo</a> and give it a star if you find it useful ⭐️</p> </article> <br /><br /> <footer> <div class="inner-footer"> <p><b>Questions? Comments? Corrections?</b></p> <br /> <p>Whatever it may be, don't hesitate to let me know. You can get in touch on <a href="https://twitter.com/augustwester" target="_blank">Twitter</a> or via <a href="mailto:august.wester@gmail.com">email</a>.</p> </div> </footer> </div> </main> </div> </div> <script> function toggleSidebar() { let content = document.getElementById("content"); let sidebar = document.getElementById("sidebar"); let overlay = document.getElementById("overlay"); let hamburger = document.getElementById("hamburger"); if (content.classList.contains("translate-right")) { content.classList.remove("translate-right"); sidebar.classList.remove("translate-right"); content.style.pointerEvents = "auto"; overlay.style.cursor = "auto"; document.body.style.overflow = "visible"; overlay.onmouseup = undefined; } else { content.classList.add("translate-right"); content.style.pointerEvents = "none"; overlay.style.cursor = "pointer"; sidebar.classList.add("translate-right"); content.style.pointerEvents = "none"; document.body.style.overflow = "hidden"; overlay.onmouseup = function() { toggleSidebar(); } } } </script> <script type="text/javascript"> var config = [{left:"€€", right:"€€", display:true}, {left: "$", right:"$", display:false}]; renderMathInElement(document.body, {delimiters: config}); </script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-19BMGFVEEX"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-19BMGFVEEX'); </script> </body> </html>
